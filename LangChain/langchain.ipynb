{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099af816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongyingyue/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎\n",
      "感谢您选择了具有优良安全性、舒适性、动力性和经济性的Lynk & Co领克汽车。\n",
      "首次使用前请仔细、完整地阅读本手册内容，将有助于您更好地了解和使用车辆。\n",
      "本手册中的所有资料均为出版时的最新资料，但本公司将对产品进行不断的改进和优化，您所购的车辆可能与本手册中的描述有所不同，请以实际\n",
      "接收的车辆为准。\n",
      "如您有任何问题，或需要预约服务，请拨打电话4006-010101 联系我们。您也可以开车前往Lynk & Co领克中心。\n",
      "在抵达之前，请您注意驾车安全。\n",
      "© 领克汽车销售有限公司\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(\"train_a.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(pages[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7563e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc1b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f66343",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"check.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(pages[2].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6d595a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 1. Join all pages into one big string\n",
    "full_text = \"\\n\".join([doc.page_content for doc in pages])\n",
    "\n",
    "# 2. Wrap it as a single Document\n",
    "combined_doc = Document(page_content=full_text)\n",
    "\n",
    "# 3. Apply text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents([combined_doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ee80dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf5ec58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示屏中点击 ，进入驾驶员侧座椅加热控制界面。\n",
      "01 设置驾驶员侧座椅加热强度及开关控制。\n",
      "在中央显示屏中点击 ，进入副驾驶员侧座椅加热控制界面。\n",
      "\n",
      "安全出行\n",
      "115\n",
      "01 设置副驾驶员侧座椅加热强度及开关控制。\n",
      "驾驶员/副驾驶员侧座椅加热分三级调节，点击控制开关后在“关-低\u0002中-高”之间循环。\n",
      "警告！\n",
      "■ 如果您或者车上的乘客（例如：病人、残疾人、无身体知觉的人\n",
      "等）身体无法感知座椅温度，请勿使用前排座椅加热功能。\n",
      "使用Lynk & Co App打开/关闭前排座椅加热\n",
      "打开/关闭前排座椅加热图标：登录Lynk & Co App，按下\n",
      "该图标可以打开/关闭前排座椅加热。\n",
      "设置前排座椅加热时间\n",
      "在\n",
      "------\n",
      "：登录Lynk & Co App，按下\n",
      "该图标可以打开/关闭前排座椅加热。\n",
      "设置前排座椅加热时间\n",
      "在中央显示屏中唤起空调控制界面，然后点击 -舒适。\n",
      "01 设置驾驶员侧座椅加热时间（5 分钟、15 分钟、30 分钟和持\n",
      "续）。\n",
      "02设置副驾驶员侧座椅加热时间（5 分钟、15 分钟、30 分钟和持\n",
      "续）。\n",
      "前排座椅通风\n",
      "通过空调辅助功能菜单调节\n",
      "您可以通过中央显示屏空调功能菜单，设置驾驶员/副驾驶员侧座椅\n",
      "通风强度或关闭座椅通风功能。\n",
      "\n",
      "安全出行\n",
      "116\n",
      "在中央显示屏中点击 ，进入驾驶员侧座椅通风控制界面。\n",
      "01 设置驾驶员侧座椅通风强度及开关控制。\n",
      "在中央显示屏中点击 ，进入副驾驶员侧座椅通风\n",
      "------\n",
      "控制界面。\n",
      "01 设置驾驶员侧座椅通风强度及开关控制。\n",
      "在中央显示屏中点击 ，进入副驾驶员侧座椅通风控制界面。\n",
      "01 设置副驾驶员侧座椅通风强度及开关控制。\n",
      "驾驶员/副驾驶员侧座椅通风分三级调节，点击控制开关后在“关-低\u0002中-高”之间循环。\n",
      "使用Lynk & Co App打开/关闭前排座椅通风\n",
      "打开/关闭前排座椅通风图标：登录Lynk & Co App，按下\n",
      "该图标可以打开/关闭前排座椅通风。\n",
      "设置前排座椅通风时间\n",
      "在中央显示屏中唤起空调控制界面，然后点击 -舒适。\n",
      "\n",
      "安全出行\n",
      "117\n",
      "01 设置驾驶员侧座椅通风时间（5 分钟、15 分钟、30 分钟和持\n",
      "续）。\n",
      "02设置副驾驶员侧座椅通风时间\n"
     ]
    }
   ],
   "source": [
    "print(chunks[200].page_content)\n",
    "print('------')\n",
    "print(chunks[201].page_content)\n",
    "print('------')\n",
    "print(chunks[202].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06625712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/j9k2b3n537bg1fr81hb9b5jh0000gn/T/ipykernel_49349/1742550774.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc45ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2905cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3d67c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = './chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66577b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b88d35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"空调加热怎么操作？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8933c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d4ef94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地优化空调的工作性能，请关闭车窗和天窗。\n",
      "您也可以通过操作中央显示屏启用/停用空调制冷。\n",
      "在中央显示屏上唤起空调控制界面。\n",
      "01 点击选择空调控制模式。\n",
      "\n",
      "空调\n",
      "246\n",
      "开启/关闭空调自动模式（AUTO）\n",
      "您可以通过空调系统自动控制功能，将车内温度设定在您认为最理想\n",
      "的水平。设定好后，系统会自动控制气流温度、气流分布和风速，将\n",
      "车内温度维持在您设定的温度值。\n",
      "出现以下一种情况后，自动模式会停用：\n",
      "■ 启用了最大除霜功能。\n",
      "■ 调节气流分布。\n",
      "您也可以通过操作中央显示屏启用/停用自动模式。\n",
      "在中央显示屏上唤起空调控制界面。\n",
      "01 点击选择空调自动控制模式。\n",
      "开启/关闭空调经济模式（ECO）\n",
      "您可\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e87369",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "217b5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11e8ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/j9k2b3n537bg1fr81hb9b5jh0000gn/T/ipykernel_49349/3864566654.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = './chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4ae2900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d5e1217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44b7cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/j9k2b3n537bg1fr81hb9b5jh0000gn/T/ipykernel_49349/60327743.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=llm_name, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e570ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49bf19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过Lynk & Co App登录，按下图标可以打开/关闭前排座椅通风。在中央显示屏中唤起空调控制界面，然后点击舒适来设置前排座椅通风时间。谢谢提问！\n"
     ]
    }
   ],
   "source": [
    "question = \"座椅通风怎么用?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d36ba8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要操作空调加热功能，您可以通过中央显示屏的空调控制界面进行设置。首先，唤起空调控制界面，然后选择相应的选项：\n",
      "1. 设置前排座椅加热时间：在舒适设置中，选择驾驶员侧和副驾驶员侧座椅加热时间（5分钟、15分钟、30分钟或持续）。\n",
      "2. 设置前排座椅通风：通过空调辅助功能菜单，可以设置驾驶员和副驾驶员侧座椅通风强度或关闭座椅通风功能。\n",
      "3. 启用方向盘加热：在空调控制界面中，点击设置方向盘加热强度及开关控制，选择低、中、高三级调节。\n",
      "4. 开启/关闭空调自动模式（AUTO）：通过空调自动控制功能，系统会自动控制气流温度、气流分布和风速，将车内温度维持在您设定的温度值。\n",
      "5. 开启/关闭空调经济模式（ECO）：可以提高驾驶环境舒适度，通过空调控制界面进行操作。\n",
      "\n",
      "请注意，具体操作步骤可能会因车型和系统设置而有所不同。\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7fe72df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. /\n",
    "Use the source language to return the answer.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. /\n",
    "Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef5de12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3a6f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"空调加热怎么操作？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48553969",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "734b0522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要操作空调加热功能，首先登录Lynk & Co App，然后按下相应图标打开/关闭座椅加热或方向盘加热。在中央显示屏中唤起空调控制界面，选择相应的设置选项即可。谢谢您的提问！\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7af7a7",
   "metadata": {},
   "source": [
    "# Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90197b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Force use of slow tokenizer to avoid SentencePiece conversion issues\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mMT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mgoogle/mt5-small\u001b[39m\u001b[33m\"\u001b[39m, use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m model = MT5ForConditionalGeneration.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/mt5-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mtext2text-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer, device=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1840\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1828\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1826\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nMT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Force use of slow tokenizer to avoid SentencePiece conversion issues\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", use_fast=False)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "response = llm(\"空调加热怎么操作？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77dea4e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ./models/llama-2-7b.ggmlv3.q4_0.bin. Received error Model path does not exist: ./models/llama-2-7b.ggmlv3.q4_0.bin [type=value_error, input_value={'model_path': './models/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# # 1. Load and split documents\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# loader = TextLoader(\"your_data.txt\")\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# documents = loader.load()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 3. Load local model (e.g., compiled LLaMA model via llama.cpp)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m llm = \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models/llama-2-7b.ggmlv3.q4_0.bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 4. RAG pipeline\u001b[39;00m\n\u001b[32m     22\u001b[39m qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ./models/llama-2-7b.ggmlv3.q4_0.bin. Received error Model path does not exist: ./models/llama-2-7b.ggmlv3.q4_0.bin [type=value_error, input_value={'model_path': './models/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# # 1. Load and split documents\n",
    "# loader = TextLoader(\"your_data.txt\")\n",
    "# documents = loader.load()\n",
    "# splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "# docs = splitter.split_documents(documents)\n",
    "\n",
    "# # 2. Embed documents locally\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# db = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "# 3. Load local model (e.g., compiled LLaMA model via llama.cpp)\n",
    "llm = LlamaCpp(model_path=\"./models/llama-2-7b.ggmlv3.q4_0.bin\")\n",
    "\n",
    "# 4. RAG pipeline\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
    "response = qa_chain.run(\"空调加热怎么操作？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "25102c0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ./models/llama-2-7b-chat.Q4_K_M.gguf. Received error Model path does not exist: ./models/llama-2-7b-chat.Q4_K_M.gguf [type=value_error, input_value={'model_path': './models/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 3. Load local model (must be .gguf)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m llm = \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models/llama-2-7b-chat.Q4_K_M.gguf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# update path\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ./models/llama-2-7b-chat.Q4_K_M.gguf. Received error Model path does not exist: ./models/llama-2-7b-chat.Q4_K_M.gguf [type=value_error, input_value={'model_path': './models/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# 3. Load local model (must be .gguf)\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\",  # update path\n",
    "    temperature=0.7,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a81fc47b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Force use of slow tokenizer to avoid SentencePiece conversion issues\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mMT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mgoogle/mt5-small\u001b[39m\u001b[33m\"\u001b[39m, use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m model = MT5ForConditionalGeneration.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/mt5-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mtext2text-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer, device=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1840\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Machine-learning-study/ml-venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:1828\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1826\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nMT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Force use of slow tokenizer to avoid SentencePiece conversion issues\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", use_fast=False)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "response = llm(\"空调加热怎么操作？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b860b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
