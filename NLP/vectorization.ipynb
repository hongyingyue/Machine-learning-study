{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e29246c",
   "metadata": {},
   "source": [
    "# Vetorization Techniques\n",
    "| Method       | Unit of Representation                           | Captures Semantics? | Dense or Sparse |\n",
    "| ------------ | ------------------------------------------------ | ------------------- | --------------- |\n",
    "| **BoW**      | Document → vector (word counts)                  | ❌ No                | Sparse          |\n",
    "| **TF-IDF**   | Document → vector (weighted counts)              | ❌ No                | Sparse          |\n",
    "| **n-grams**  | Document → vector of n-word sequences            | ❌ No                | Sparse          |\n",
    "| **Word2Vec** | Word → dense vector (pretrained or learned)      | ✅ Yes               | Dense           |\n",
    "| **Doc2Vec**  | Document → dense vector                          | ✅ Yes               | Dense           |\n",
    "| **BERT**     | Word, sentence, or doc → context-aware embedding | ✅ Yes               | Dense           |\n",
    "\n",
    "Sparse vectors are **Counting-based**, while Dense vectors are **Prediction-based**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa181b",
   "metadata": {},
   "source": [
    "## BoW\n",
    "Bag of Words (BoW)is a vectorization technique that:\n",
    "- Takes tokenized words (usually from a word-level tokenizer)\n",
    "  \n",
    "- Builds a vocabulary (set of unique tokens)\n",
    "  \n",
    "- Converts each document into a vector counting how many times each word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c9169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary: ['and' 'coding' 'exciting' 'fun' 'in' 'is' 'learning' 'love' 'machine'\n",
      " 'nlp' 'python']\n",
      "   and  coding  exciting  fun  in  is  learning  love  machine  nlp  python\n",
      "0    1       0         0    0   0   0         1     1        1    1       0\n",
      "1    1       0         1    1   0   1         0     0        0    1       0\n",
      "2    0       1         0    0   1   0         0     1        0    0       1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "docs = [\n",
    "    \"I love NLP and machine learning\",\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Create BoW vectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(docs)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"BoW Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# BoW Matrix\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178bd1c",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) is a vectorization technique that:\n",
    "- Takes tokenized words from each document\n",
    "  \n",
    "- Builds a vocabulary (like BoW), but weights each word by its importance\n",
    "  \n",
    "- Calculates TF (how often a word appears in a document)\n",
    "  \n",
    "- Calculates IDF (how rare the word is across all documents)\n",
    "  \n",
    "- Converts each document into a vector where frequent and rare (but informative) words get higher scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce2f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'coding' 'exciting' 'fun' 'in' 'is' 'learning' 'love' 'machine'\n",
      " 'nlp' 'python']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>coding</th>\n",
       "      <th>exciting</th>\n",
       "      <th>fun</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "      <th>python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.402040</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and    coding  exciting       fun        in        is  learning  \\\n",
       "0  0.393511  0.000000  0.000000  0.000000  0.000000  0.000000   0.51742   \n",
       "1  0.373022  0.000000  0.490479  0.490479  0.000000  0.490479   0.00000   \n",
       "2  0.000000  0.528635  0.000000  0.000000  0.528635  0.000000   0.00000   \n",
       "\n",
       "       love  machine       nlp    python  \n",
       "0  0.393511  0.51742  0.393511  0.000000  \n",
       "1  0.000000  0.00000  0.373022  0.000000  \n",
       "2  0.402040  0.00000  0.000000  0.528635  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"I love NLP and machine learning\",\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Vocabulary:\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF Matrix:\n",
    "import pandas as pd\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f490af7",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "n-grams is a vectorization technique that:\n",
    "- Extracts continuous sequences of n words (e.g., bigrams = 2 words, trigrams = 3) from each document\n",
    "\n",
    "- Builds a vocabulary of all observed n-grams\n",
    "\n",
    "- Converts each document into a vector by counting how many times each n-gram appears\n",
    "\n",
    "- Captures limited word order or phrase patterns, unlike BoW and TF-IDF which treat words independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c39827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Vocabulary: ['and exciting' 'and machine' 'coding in' 'fun and' 'in python' 'is fun'\n",
      " 'love coding' 'love nlp' 'machine learning' 'nlp and' 'nlp is']\n",
      "   and exciting  and machine  coding in  fun and  in python  is fun  \\\n",
      "0             0            1          0        0          0       0   \n",
      "1             1            0          0        1          0       1   \n",
      "2             0            0          1        0          1       0   \n",
      "\n",
      "   love coding  love nlp  machine learning  nlp and  nlp is  \n",
      "0            0         1                 1        1       0  \n",
      "1            0         0                 0        0       1  \n",
      "2            1         0                 0        0       0  \n"
     ]
    }
   ],
   "source": [
    "# Create n-gram vectorizer (bigrams)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bigrams = bigram_vectorizer.fit_transform(docs)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Bigram Matrix\n",
    "bigram_df = pd.DataFrame(X_bigrams.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "print(bigram_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b865d0e",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is a vectorization technique that:\n",
    "- Learns dense vector representations of individual words based on surrounding word context\n",
    "\n",
    "- Uses architectures like CBOW (predict word from context) or Skip-gram (predict context from word)\n",
    "\n",
    "- Embeds similar words close together in vector space (e.g., “king” and “queen”)\n",
    "\n",
    "- Converts text into word vectors, often averaged or pooled to represent entire sentences/documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0dc069",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Doc2Vec is a vectorization technique that:\n",
    "- Extends Word2Vec to learn dense vector representations for entire documents or paragraphs\n",
    "\n",
    "- Associates each document with a unique ID, which is trained along with word vectors\n",
    "\n",
    "- Captures document-level semantics rather than averaging word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555025bd",
   "metadata": {},
   "source": [
    "## BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a vectorization technique that:\n",
    "- Uses a pretrained Transformer model to generate context-aware dense embeddings\n",
    "\n",
    "- Represents words, sentences, or entire documents based on their surrounding context\n",
    "\n",
    "- Unlike Word2Vec or TF-IDF, BERT considers the meaning of a word in its sentence (e.g., \"bank\" in finance vs river)\n",
    "\n",
    "- Outputs high-dimensional vectors (e.g., 768-dim) that can be used for classification, similarity, clustering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1fab3",
   "metadata": {},
   "source": [
    "## 🔍 Expanded Comparison of Text Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af6d80",
   "metadata": {},
   "source": [
    "| Feature                   | **BoW**                           | **TF-IDF**                         | **n-grams**                     | **Word2Vec**                              | **Doc2Vec**                      | **BERT**                                            |\n",
    "| ------------------------- | --------------------------------- | ---------------------------------- | ------------------------------- | ----------------------------------------- | -------------------------------- | --------------------------------------------------- |\n",
    "| **Granularity**           | Document                          | Document                           | Document                        | Word                                      | Document                         | Word, Sentence, or Document                         |\n",
    "| **Type**                  | Sparse                            | Sparse                             | Sparse                          | Dense                                     | Dense                            | Dense                                               |\n",
    "| **Captures Word Order?**  | ❌ No                              | ❌ No                               | ✅ Partial (via sequence)        | ❌ No (context window only)                | ✅ Some via training context      | ✅ Yes (via Transformer attention)                   |\n",
    "| **Captures Context?**     | ❌ No                              | ❌ No                               | ❌ No                            | ✅ Local context                           | ✅ Global context                 | ✅ Deep contextual understanding                     |\n",
    "| **Vocabulary-Dependent?** | ✅ Yes                             | ✅ Yes                              | ✅ Yes                           | ✅ Yes                                     | ✅ Yes                            | ❌ No (tokenizer dependent, but model is contextual) |\n",
    "| **Handles Polysemy?**     | ❌ No                              | ❌ No                               | ❌ No                            | ❌ No                                      | ❌ No                             | ✅ Yes (context-specific embeddings)                 |\n",
    "| **Trainable?**            | ❌ No (purely statistical)         | ❌ No                               | ❌ No                            | ✅ Yes (unsupervised pretraining)          | ✅ Yes (unsupervised pretraining) | ✅ Yes (pretrained + fine-tuning optional)           |\n",
    "| **Interpretability**      | ✅ High (clear term mapping)       | ✅ Moderate (weighted terms)        | ✅ Moderate                      | ❌ Low (black-box embeddings)              | ❌ Low                            | ❌ Low                                               |\n",
    "| **Dimensionality**        | High (vocab size)                 | High (vocab size)                  | High (vocab size of n-grams)    | Lower (e.g., 100–300)                     | Lower (e.g., 100–300)            | Medium–High (e.g., 768, 1024)                       |\n",
    "| **Memory Efficient?**     | ❌ No                              | ❌ No                               | ❌ No                            | ✅ Yes                                     | ✅ Yes                            | ❌ Moderate–High                                     |\n",
    "| **Typical Use Cases**     | Baseline NLP, text classification | IR, keyword extraction, clustering | Phrase detection, n-gram models | Semantic similarity, clustering, NN input | Document similarity, retrieval   | QA, summarization, classification, RAG, etc.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc73f4",
   "metadata": {},
   "source": [
    "### 🔑 Quick Summary\n",
    "BoW/TF-IDF/n-grams:\n",
    "\n",
    "- Simple, interpretable, but lose meaning and context\n",
    "\n",
    "- Best for classical ML (SVM, NB, LR) on small to medium corpora\n",
    "\n",
    "Word2Vec/Doc2Vec:\n",
    "\n",
    "- Introduce semantics via learned vectors\n",
    "\n",
    "- Suitable for downstream ML tasks like clustering, similarity, recommendation\n",
    "\n",
    "BERT:\n",
    "\n",
    "- Most powerful and flexible\n",
    "\n",
    "- Context-aware, pretrained, adaptable to almost any NLP task\n",
    "\n",
    "- High cost but highest performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
